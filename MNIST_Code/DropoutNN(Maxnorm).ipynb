{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1024 neurons in each hidden layers\n",
    "n_hidden_1 = 1024\n",
    "n_hidden_2 = 1024\n",
    "n_hidden_3 = 1024\n",
    "\n",
    "# input size is the size of a picture: 28*28\n",
    "# output size\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.0005\n",
    "training_epochs = 100\n",
    "batch_size = 200\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape):\n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    weight_init = tf.random_normal_initializer(stddev=(2.0/weight_shape[0])**0.5)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init,constraint = tf.keras.constraints.MaxNorm())\n",
    "    \n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    \n",
    "    return tf.nn.relu(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x, keep_prob):\n",
    "    \"\"\"\n",
    "    define the structure of the whole network\n",
    "    input:\n",
    "        - x: a batch of pictures \n",
    "        (input shape = (batch_size*image_size))\n",
    "        - keep_prob: The keep_prob of dropout layer\n",
    "    output:\n",
    "        - a batch vector corresponding to the logits predicted by the network\n",
    "        (output shape = (batch_size*output_size)) \n",
    "    \"\"\"\n",
    "    if keep_prob == 1:\n",
    "        x = tf.nn.dropout(x,keep_prob)\n",
    "    else:\n",
    "        x = tf.nn.dropout(x, 0.8)\n",
    "    x = tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "    with tf.variable_scope(\"fully_connected1\"):\n",
    "        \n",
    "        # pass the output of max-pooling into a Fully_Connected layer\n",
    "        x = tf.reshape(x,[-1,28*28])\n",
    "        # after reshaping, use fully-connected layer to compress\n",
    "        fc_1 = layer(x, [28*28, n_hidden_1], [n_hidden_1])\n",
    "        \n",
    "        # apply dropout. You may try to add drop out after every pooling layer.\n",
    "        # outputs the input element scaled up by 1/keep_prob\n",
    "        # The scaling is so that the expected sum is unchanged\n",
    "        fc_1_drop = tf.nn.dropout(fc_1, keep_prob)\n",
    "    \n",
    "    with tf.variable_scope(\"fully_connected2\"):\n",
    "        \n",
    "        # pass the output of max-pooling into a Fully_Connected layer\n",
    "\n",
    "        # after reshaping, use fully-connected layer to compress\n",
    "        fc_2 = layer(fc_1_drop, [n_hidden_1, n_hidden_2], [n_hidden_2])\n",
    "        \n",
    "        # apply dropout. You may try to add drop out after every pooling layer.\n",
    "        # outputs the input element scaled up by 1/keep_prob\n",
    "        # The scaling is so that the expected sum is unchanged\n",
    "        fc_2_drop = tf.nn.dropout(fc_2, keep_prob)\n",
    "        \n",
    "    with tf.variable_scope(\"fully_connected3\"):\n",
    "        \n",
    "        # pass the output of max-pooling into a Fully_Connected layer\n",
    "\n",
    "        # after reshaping, use fully-connected layer to compress\n",
    "        fc_3 = layer(fc_2_drop, [n_hidden_2, n_hidden_3], [n_hidden_3])\n",
    "        \n",
    "        # apply dropout. You may try to add drop out after every pooling layer.\n",
    "        # outputs the input element scaled up by 1/keep_prob\n",
    "        # The scaling is so that the expected sum is unchanged\n",
    "        fc_3_drop = tf.nn.dropout(fc_3, keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(fc_3_drop, [n_hidden_3, 10], [10])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    \"\"\"\n",
    "    Computes softmax cross entropy between logits and labels and then the loss \n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)    \n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    defines the necessary elements to train the network\n",
    "    \n",
    "    intput:\n",
    "        - cost: the cost is the loss of the corresponding batch\n",
    "        - global_step: number of batch seen so far, it is incremented by one each time the .minimize() function is called\n",
    "    \"\"\"\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    # using Adam Optimizer \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    #grads = optimizer.compute_gradients(cost)\n",
    "    #for i, (g,v) in enumerate(grads):\n",
    "    #    grads[i] = (tf.clip_by_norm(g,3.5),v)\n",
    "    #train_op = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy on the validation set \n",
    "    input:\n",
    "        -output: prediction vector of the network for the validation set\n",
    "        -y: true value for the validation set\n",
    "    output:\n",
    "        - accuracy: accuracy on the validation set (scalar between 0 and 1)\n",
    "    \"\"\"\n",
    "    #correct prediction is a binary vector which equals one when the output and y match\n",
    "    #otherwise the vector equals 0\n",
    "    #tf.cast: change the type of a tensor into another one\n",
    "    #then, by taking the mean of the tensor, we directly have the average score, so the accuracy\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"validation_error\", (1.0 - accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.899320444\n",
      "Validation Error: 0.06279999017715454\n",
      "Epoch: 0002 cost = 0.282273567\n",
      "Validation Error: 0.040400028228759766\n",
      "Epoch: 0003 cost = 0.210046640\n",
      "Validation Error: 0.03780001401901245\n",
      "Epoch: 0004 cost = 0.178982695\n",
      "Validation Error: 0.031199991703033447\n",
      "Epoch: 0005 cost = 0.154254755\n",
      "Validation Error: 0.029600024223327637\n",
      "Epoch: 0006 cost = 0.136103214\n",
      "Validation Error: 0.02679997682571411\n",
      "Epoch: 0007 cost = 0.125222788\n",
      "Validation Error: 0.025399982929229736\n",
      "Epoch: 0008 cost = 0.113933550\n",
      "Validation Error: 0.024200022220611572\n",
      "Epoch: 0009 cost = 0.105201631\n",
      "Validation Error: 0.025799989700317383\n",
      "Epoch: 0010 cost = 0.099740214\n",
      "Validation Error: 0.024200022220611572\n",
      "Epoch: 0011 cost = 0.093429579\n",
      "Validation Error: 0.02319997549057007\n",
      "Epoch: 0012 cost = 0.089445092\n",
      "Validation Error: 0.02120000123977661\n",
      "Epoch: 0013 cost = 0.085177386\n",
      "Validation Error: 0.021399974822998047\n",
      "Epoch: 0014 cost = 0.078672063\n",
      "Validation Error: 0.016799986362457275\n",
      "Epoch: 0015 cost = 0.074254803\n",
      "Validation Error: 0.02060002088546753\n",
      "Epoch: 0016 cost = 0.072962021\n",
      "Validation Error: 0.021000027656555176\n",
      "Epoch: 0017 cost = 0.071839297\n",
      "Validation Error: 0.02120000123977661\n",
      "Epoch: 0018 cost = 0.068254169\n",
      "Validation Error: 0.01940000057220459\n",
      "Epoch: 0019 cost = 0.063977950\n",
      "Validation Error: 0.021600008010864258\n",
      "Epoch: 0020 cost = 0.061938504\n",
      "Validation Error: 0.017000019550323486\n",
      "Epoch: 0021 cost = 0.060019966\n",
      "Validation Error: 0.019999980926513672\n",
      "Epoch: 0022 cost = 0.058478981\n",
      "Validation Error: 0.017799973487854004\n",
      "Epoch: 0023 cost = 0.056931280\n",
      "Validation Error: 0.019800007343292236\n",
      "Epoch: 0024 cost = 0.056903690\n",
      "Validation Error: 0.019800007343292236\n",
      "Epoch: 0025 cost = 0.054088035\n",
      "Validation Error: 0.01819998025894165\n",
      "Epoch: 0026 cost = 0.054504388\n",
      "Validation Error: 0.01940000057220459\n",
      "Epoch: 0027 cost = 0.050512046\n",
      "Validation Error: 0.01819998025894165\n",
      "Epoch: 0028 cost = 0.052345238\n",
      "Validation Error: 0.017400026321411133\n",
      "Epoch: 0029 cost = 0.050736785\n",
      "Validation Error: 0.01819998025894165\n",
      "Epoch: 0030 cost = 0.046566147\n",
      "Validation Error: 0.016200006008148193\n",
      "Epoch: 0031 cost = 0.049207802\n",
      "Validation Error: 0.018599987030029297\n",
      "Epoch: 0032 cost = 0.043946490\n",
      "Validation Error: 0.018800020217895508\n",
      "Epoch: 0033 cost = 0.046408723\n",
      "Validation Error: 0.014999985694885254\n",
      "Epoch: 0034 cost = 0.043661658\n",
      "Validation Error: 0.017199993133544922\n",
      "Epoch: 0035 cost = 0.045094935\n",
      "Validation Error: 0.018800020217895508\n",
      "Epoch: 0036 cost = 0.043492038\n",
      "Validation Error: 0.017000019550323486\n",
      "Epoch: 0037 cost = 0.042592107\n",
      "Validation Error: 0.015600025653839111\n",
      "Epoch: 0038 cost = 0.041404506\n",
      "Validation Error: 0.018999993801116943\n",
      "Epoch: 0039 cost = 0.042223253\n",
      "Validation Error: 0.017199993133544922\n",
      "Epoch: 0040 cost = 0.039196097\n",
      "Validation Error: 0.018000006675720215\n",
      "Epoch: 0041 cost = 0.038775084\n",
      "Validation Error: 0.017000019550323486\n",
      "Epoch: 0042 cost = 0.039015761\n",
      "Validation Error: 0.01819998025894165\n",
      "Epoch: 0043 cost = 0.036504747\n",
      "Validation Error: 0.0153999924659729\n",
      "Epoch: 0044 cost = 0.039768078\n",
      "Validation Error: 0.015999972820281982\n",
      "Epoch: 0045 cost = 0.038579442\n",
      "Validation Error: 0.017000019550323486\n",
      "Epoch: 0046 cost = 0.036819252\n",
      "Validation Error: 0.017000019550323486\n",
      "Epoch: 0047 cost = 0.035502851\n",
      "Validation Error: 0.017400026321411133\n",
      "Epoch: 0048 cost = 0.038018095\n",
      "Validation Error: 0.017199993133544922\n",
      "Epoch: 0049 cost = 0.036576221\n",
      "Validation Error: 0.01840001344680786\n",
      "Epoch: 0050 cost = 0.033562887\n",
      "Validation Error: 0.0153999924659729\n",
      "Epoch: 0051 cost = 0.034870360\n",
      "Validation Error: 0.017000019550323486\n",
      "Epoch: 0052 cost = 0.035041290\n",
      "Validation Error: 0.014800012111663818\n",
      "Epoch: 0053 cost = 0.033176186\n",
      "Validation Error: 0.018800020217895508\n",
      "Epoch: 0054 cost = 0.033963739\n",
      "Validation Error: 0.017199993133544922\n",
      "Epoch: 0055 cost = 0.032297896\n",
      "Validation Error: 0.015999972820281982\n",
      "Epoch: 0056 cost = 0.035276056\n",
      "Validation Error: 0.017000019550323486\n",
      "Epoch: 0057 cost = 0.032569380\n",
      "Validation Error: 0.016799986362457275\n",
      "Epoch: 0058 cost = 0.034217517\n",
      "Validation Error: 0.01380002498626709\n",
      "Epoch: 0059 cost = 0.033531607\n",
      "Validation Error: 0.01639997959136963\n",
      "Epoch: 0060 cost = 0.030697821\n",
      "Validation Error: 0.017199993133544922\n",
      "Epoch: 0061 cost = 0.032702788\n",
      "Validation Error: 0.017000019550323486\n",
      "Epoch: 0062 cost = 0.033374018\n",
      "Validation Error: 0.01759999990463257\n",
      "Epoch: 0063 cost = 0.032573497\n",
      "Validation Error: 0.017799973487854004\n",
      "Epoch: 0064 cost = 0.032874233\n",
      "Validation Error: 0.0153999924659729\n",
      "Epoch: 0065 cost = 0.033689166\n",
      "Validation Error: 0.014400005340576172\n",
      "Epoch: 0066 cost = 0.029180011\n",
      "Validation Error: 0.014999985694885254\n",
      "Epoch: 0067 cost = 0.030423683\n",
      "Validation Error: 0.015200018882751465\n",
      "Epoch: 0068 cost = 0.031080084\n",
      "Validation Error: 0.01759999990463257\n",
      "Epoch: 0069 cost = 0.030045871\n",
      "Validation Error: 0.015999972820281982\n",
      "Epoch: 0070 cost = 0.028694406\n",
      "Validation Error: 0.014400005340576172\n",
      "Epoch: 0071 cost = 0.029736496\n",
      "Validation Error: 0.014999985694885254\n",
      "Epoch: 0072 cost = 0.029360478\n",
      "Validation Error: 0.017199993133544922\n",
      "Epoch: 0073 cost = 0.028823499\n",
      "Validation Error: 0.014400005340576172\n",
      "Epoch: 0074 cost = 0.028249844\n",
      "Validation Error: 0.0153999924659729\n",
      "Epoch: 0075 cost = 0.029305911\n",
      "Validation Error: 0.015799999237060547\n",
      "Epoch: 0076 cost = 0.029091591\n",
      "Validation Error: 0.016799986362457275\n",
      "Epoch: 0077 cost = 0.028890097\n",
      "Validation Error: 0.01660001277923584\n",
      "Epoch: 0078 cost = 0.030156439\n",
      "Validation Error: 0.01380002498626709\n",
      "Epoch: 0079 cost = 0.029918403\n",
      "Validation Error: 0.015200018882751465\n",
      "Epoch: 0080 cost = 0.027634436\n",
      "Validation Error: 0.016799986362457275\n",
      "Epoch: 0081 cost = 0.029179333\n",
      "Validation Error: 0.01639997959136963\n",
      "Epoch: 0082 cost = 0.029227455\n",
      "Validation Error: 0.013000011444091797\n",
      "Epoch: 0083 cost = 0.025716296\n",
      "Validation Error: 0.016200006008148193\n",
      "Epoch: 0084 cost = 0.028301272\n",
      "Validation Error: 0.016799986362457275\n",
      "Epoch: 0085 cost = 0.027907578\n",
      "Validation Error: 0.015600025653839111\n",
      "Epoch: 0086 cost = 0.029174010\n",
      "Validation Error: 0.018000006675720215\n",
      "Epoch: 0087 cost = 0.027429913\n",
      "Validation Error: 0.013199985027313232\n",
      "Epoch: 0088 cost = 0.027146144\n",
      "Validation Error: 0.016799986362457275\n",
      "Epoch: 0089 cost = 0.025620686\n",
      "Validation Error: 0.0153999924659729\n",
      "Epoch: 0090 cost = 0.028015596\n",
      "Validation Error: 0.0153999924659729\n",
      "Epoch: 0091 cost = 0.027067781\n",
      "Validation Error: 0.014800012111663818\n",
      "Epoch: 0092 cost = 0.027796582\n",
      "Validation Error: 0.014199972152709961\n",
      "Epoch: 0093 cost = 0.025981542\n",
      "Validation Error: 0.013999998569488525\n",
      "Epoch: 0094 cost = 0.026212239\n",
      "Validation Error: 0.015200018882751465\n",
      "Epoch: 0095 cost = 0.026621384\n",
      "Validation Error: 0.017000019550323486\n",
      "Epoch: 0096 cost = 0.027468728\n",
      "Validation Error: 0.013999998569488525\n",
      "Epoch: 0097 cost = 0.025859627\n",
      "Validation Error: 0.0153999924659729\n",
      "Epoch: 0098 cost = 0.024848938\n",
      "Validation Error: 0.01380002498626709\n",
      "Epoch: 0099 cost = 0.027552440\n",
      "Validation Error: 0.01239997148513794\n",
      "Epoch: 0100 cost = 0.025656452\n",
      "Validation Error: 0.013400018215179443\n",
      "Optimization Done\n",
      "Test Accuracy: 0.984\n",
      "Execution time was 4232.650\n"
     ]
    }
   ],
   "source": [
    "earlystop_cnt = 0\n",
    "earlystop_threshold = 16\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #please, make sure you changed for your own path \n",
    "    log_files_path = 'C:/Users/WeiLiu/logs/CNNs/'\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.variable_scope(\"MNIST_DropoutNNRelu_model\"):\n",
    "            #neural network definition\n",
    "            \n",
    "            #the input variables are first define as placeholder \n",
    "            # a placeholder is a variable/data which will be assigned later \n",
    "            # MNIST data image of shape 28*28=784\n",
    "            x = tf.placeholder(\"float\", [None, 784]) \n",
    "            # 0-9 digits recognition\n",
    "            y = tf.placeholder(\"float\", [None, 10])  \n",
    "            \n",
    "            # dropout probability\n",
    "            keep_prob = tf.placeholder(tf.float32) \n",
    "            #the network is defined using the inference function defined above in the code\n",
    "            output = inference(x, keep_prob)\n",
    "            cost = loss(output, y)\n",
    "            #initialize the value of the global_step variable \n",
    "            # recall: it is incremented by one each time the .minimise() is called\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            train_op = training(cost, global_step)\n",
    "            #evaluate the accuracy of the network (done on a validation set)\n",
    "            eval_op = evaluate(output, y)\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            saver = tf.train.Saver()\n",
    "            sess = tf.Session()\n",
    "            \n",
    "            summary_writer = tf.summary.FileWriter(log_files_path, sess.graph)\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            \n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                avg_cost = 0.0\n",
    "                total_batch = int(mnist.train.num_examples/batch_size)\n",
    "                max_val_acc = 0.0\n",
    "                prev_tr_acc = 0.0\n",
    "                \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    \n",
    "                    minibatch_x, minibatch_y = mnist.train.next_batch(batch_size)\n",
    "                    \n",
    "                    # Fit training using batch data\n",
    "                    sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y, keep_prob: 0.5})\n",
    "                    \n",
    "                    # Compute average loss\n",
    "                    avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y, keep_prob: 0.5})/total_batch\n",
    "                \n",
    "                \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    \n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:0.9f}\".format(avg_cost))\n",
    "                    \n",
    "                    #probability dropout of 1 during validation\n",
    "                    accuracy_tr = sess.run(eval_op, feed_dict={x: mnist.train.images, y: mnist.train.labels, keep_prob: 0.5})\n",
    "                    accuracy_val = sess.run(eval_op, feed_dict={x: mnist.validation.images, y: mnist.validation.labels, keep_prob: 1})\n",
    "                    print(\"Validation Error:\", (1 - accuracy_val))\n",
    "                    \n",
    "                    if accuracy_val < max_val_acc:\n",
    "                        if accuracy_tr > prev_tr_acc or accuracy_tr > 0.99:\n",
    "                            if earlystop_cnt == earlystop_threshold:\n",
    "                                print(\"early stopped on\" + str(epoch))\n",
    "                                break\n",
    "                            else:\n",
    "                                print(\"overfitting warning:\" + str(earlystop_cnt))\n",
    "                                earlystop_cnt += 1\n",
    "                        else:\n",
    "                            earlystop_cnt = 0\n",
    "                    else:\n",
    "                        earlystop_cnt = 0\n",
    "                        max_val_acc = accuracy_val\n",
    "                        \n",
    "                    prev_tr_acc = accuracy_tr\n",
    "                    \n",
    "                    # probability dropout of 0.25 during training\n",
    "                    summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y, keep_prob: 0.5})\n",
    "                    summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "                    \n",
    "                    saver.save(sess, log_files_path+'model-checkpoint', global_step=global_step)\n",
    "                    \n",
    "            print(\"Optimization Done\")\n",
    "                    \n",
    "            accuracy = sess.run(eval_op, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1})\n",
    "            print(\"Test Accuracy:\", accuracy)\n",
    "                    \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Execution time was %0.3f' % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
